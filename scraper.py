import feedparser
import json
import random
from datetime import datetime

# å®šç¾© 7 å¤§åˆ†é¡åŠå…¶å°æ‡‰çš„ 3 å€‹ä»¥ä¸Šæ¬Šå¨ä¾†æº
RSS_FEEDS = {
    "å»ºç¯‰": [
        "https://www.archdaily.com/feed",
        "https://www.dezeen.com/architecture/feed/",
        "https://www.architecturalrecord.com/rss/articles"
    ],
    "å·¥æ¥­è¨­è¨ˆ": [
        "https://www.core77.com/blog/rss",
        "https://design-milk.com/category/design/feed/",
        "https://www.yankodesign.com/feed/"
    ],
    "äº’å‹•è¨­è¨ˆ": [
        "https://uxdesign.cc/feed",
        "https://www.smashingmagazine.com/feed/",
        "https://uxmagazine.com/feed/"
    ],
    "è—è¡“": [
        "https://www.designboom.com/art/feed/",
        "https://www.thisiscolossal.com/feed/",
        "https://www.juxtapoz.com/feed/"
    ],
    "å¹³é¢è¨­è¨ˆ": [
        "https://www.creativebloq.com/feed",
        "https://www.itsnicethat.com/rss",
        "https://www.printmag.com/feed/"
    ],
    "æ”å½±": [
        "https://petapixel.com/feed/",
        "https://www.dpreview.com/index.xml",
        "https://www.lensculture.com/feed"
    ],
    "ç§‘æŠ€": [
        "https://www.theverge.com/rss/index.xml",
        "https://www.wired.com/feed/rss",
        "https://techcrunch.com/feed/"
    ]
}

def scrape_news():
    all_news = []
    
    for category, urls in RSS_FEEDS.items():
        print(f"ğŸ“¡ æ­£åœ¨æŠ“å–åˆ†é¡ï¼š[{category}]")
        for url in urls:
            try:
                feed = feedparser.parse(url)
                # æ¯å€‹ç¶²ç«™å–å‰ 4 å‰‡æœ€æ–°çš„æ–‡ç« 
                for entry in feed.entries[:4]:
                    # æŠ“å–åœ–ç‰‡é‚è¼¯
                    img_url = ""
                    if 'media_content' in entry:
                        img_url = entry.media_content[0]['url']
                    elif 'enclosures' in entry and len(entry.enclosures) > 0:
                        img_url = entry.enclosures[0].href
                    
                    all_news.append({
                        "category": category,
                        "title": entry.title,
                        "link": entry.link,
                        "summary": entry.get("summary", "")[:100].strip() + "...",
                        "source": feed.feed.title if 'title' in feed.feed else "æ¬Šå¨ä¾†æº",
                        "image": img_url,
                        "date": entry.get("published", "")
                    })
            except Exception as e:
                print(f"âŒ ç„¡æ³•è®€å– {url}: {e}")

    # æ‰“äº‚é †åºï¼Œè®“ä¸åŒåˆ†é¡æ··åˆåœ¨ä¸€èµ·ï¼Œå¢åŠ é–±è®€æ¨‚è¶£
    random.shuffle(all_news)
    
    # å„²å­˜çµæœ
    with open('news.json', 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…±è¨ˆ {len(all_news)} å‰‡æ–°èã€‚")

if __name__ == "__main__":
    scrape_news()
